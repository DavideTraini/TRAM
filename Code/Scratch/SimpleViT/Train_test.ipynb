{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXC93PvZHALw"
   },
   "source": [
    "# Import e path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bBKm4O3pn19s"
   },
   "outputs": [],
   "source": [
    "results_save_path = \"../../Results/\"\n",
    "dataset_path = \"../../Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from vit_pytorch import SimpleViT\n",
    "from PatchMerger import PatchMerger\n",
    "from TRAM import TRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi1SBM-8TRqg"
   },
   "source": [
    "## Validi per ogni modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zIBVHD-TTU6u"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qL0i8NnGpUPc"
   },
   "outputs": [],
   "source": [
    "# Verifica se la GPU Ã¨ disponibile\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni per train e validation del modello RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(model, optimz, data_load, loss_val, device, scheduler):\n",
    "    samples = len(data_load.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_load):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimz.zero_grad()\n",
    "        out = functional.log_softmax(model(data), dim=1)\n",
    "        loss = functional.nll_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimz.step()\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())\n",
    "    loss_val.append(loss.item())\n",
    "\n",
    "def evaluate(model, optimizer, data_load, loss_val, device):\n",
    "    model.eval()\n",
    "\n",
    "    samples = len(data_load.dataset)\n",
    "    # predizioni corrette\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = functional.log_softmax(model(data), dim=1)\n",
    "            loss = functional.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    aloss = tloss / samples\n",
    "    loss_val.append(aloss)\n",
    "    acc = (100.0 * csamp / samples).cpu()\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(samples) + ' (' +\n",
    "          '{:4.2f}'.format(acc) + '%)\\n')\n",
    "\n",
    "    return acc\n",
    "\n",
    "def train_validation(model, optimizer, train_loader, validation_loader, dataset_name, epoche,scheduler, device):\n",
    "  tr_loss, ts_loss, ts_acc, epoch_time_list = [], [], [], []\n",
    "\n",
    "  for epoch in range(1, epoche + 1):\n",
    "\n",
    "      start_time = time.time()\n",
    "\n",
    "      print(f'Epoch: {epoch}/{epoche}')\n",
    "      print(\"INIZIO TRAINING\")\n",
    "      train_iter(model, optimizer, train_loader, tr_loss, device, scheduler= scheduler)\n",
    "      print(\"INIZIO VALIDATION\")\n",
    "      acc = evaluate(model, optimizer, validation_loader, ts_loss, device)\n",
    "\n",
    "      ts_acc.append(acc)\n",
    "\n",
    "\n",
    "      epoch_time = time.time() - start_time\n",
    "      epoch_time_list.append(epoch_time)\n",
    "\n",
    "      print('Execution time:', '{:5.2f}'.format(epoch_time), 'seconds')\n",
    "      print(\"#\"*40)\n",
    "\n",
    "  return tr_loss, ts_loss, ts_acc, epoch_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sG2VBWH-HmoI"
   },
   "source": [
    "## Funzioni per train e validation del modello SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NslZrU5FGFWo"
   },
   "outputs": [],
   "source": [
    "def train_iter_sampling(model, optimz, data_load, loss_val, device, n_patch,scheduler):\n",
    "    samples = len(data_load.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_load):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimz.zero_grad()\n",
    "        out = functional.log_softmax(model(data, n_patch), dim=1)\n",
    "        loss = functional.nll_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimz.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())\n",
    "    loss_val.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SiazmHl-QtQ_"
   },
   "outputs": [],
   "source": [
    "def evaluate_sampling(model, optimizer, data_load, loss_val, device, n_patch):\n",
    "    model.eval()\n",
    "\n",
    "    samples = len(data_load.dataset)\n",
    "    # predizioni corrette\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = functional.log_softmax(model(data, n_patch), dim=1)\n",
    "            loss = functional.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    aloss = tloss / samples\n",
    "    loss_val.append(aloss)\n",
    "    acc = (100.0 * csamp / samples).cpu()\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(samples) + ' (' +\n",
    "          '{:4.2f}'.format(acc) + '%)\\n')\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S9XmFO4NXfvR"
   },
   "outputs": [],
   "source": [
    "def train_validation_sampling(model, optimizer, train_loader, validation_loader, dataset_name, epoche, device, n_patch, scheduler):\n",
    "  tr_loss, ts_loss, ts_acc, epoch_time_list = [], [], [], []\n",
    "\n",
    "  for epoch in range(1, epoche + 1):\n",
    "\n",
    "      start_time = time.time()\n",
    "\n",
    "      print(f'Epoch: {epoch}/{epoche}')\n",
    "      print(\"INIZIO TRAINING\")\n",
    "      train_iter_sampling(model, optimizer, train_loader, tr_loss, device, n_patch, scheduler)\n",
    "      print(\"INIZIO VALIDATION\")\n",
    "      acc = evaluate_sampling(model, optimizer, validation_loader, ts_loss, device, n_patch)\n",
    "\n",
    "      ts_acc.append(acc)\n",
    "\n",
    "\n",
    "      epoch_time = time.time() - start_time\n",
    "      epoch_time_list.append(epoch_time)\n",
    "\n",
    "      print('Execution time:', '{:5.2f}'.format(epoch_time), 'seconds')\n",
    "      print(\"#\"*40)\n",
    "\n",
    "  return tr_loss, ts_loss, ts_acc, epoch_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViT_train_test_save(dataset_name, model, n_patch = None):\n",
    "    \n",
    "    model.to(device)\n",
    "    # definiamo l'ottimizzatore\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer,step_size=10, gamma=0.5)\n",
    "    initial = time.time()\n",
    "    if n_patch == None: \n",
    "        _, _, validation_acc, epoch_time = train_validation(model, optimizer, train_loader, val_loader, dataset_name, epoche, device = device, scheduler = scheduler)\n",
    "    else:\n",
    "        _, _, validation_acc, epoch_time = train_validation_sampling(model, optimizer, train_loader, val_loader, dataset_name, epoche, device = device, n_patch = n_patch, scheduler = scheduler)\n",
    "    print(f'Total Time: {time.time()-initial}')\n",
    "    \n",
    "    df = pd.DataFrame({'validation_acc': [tensor.item() for tensor in validation_acc],\n",
    "                       'epoch_time': epoch_time\n",
    "                       })\n",
    "    \n",
    "    if not os.path.exists(results_save_path):\n",
    "        os.makedirs(results_save_path)\n",
    "        \n",
    "    df.to_csv(f'{results_save_path}{dataset_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tench', 'Tinca tinca'), ('English springer', 'English springer spaniel'), ('cassette player',), ('chain saw', 'chainsaw'), ('church', 'church building'), ('French horn', 'horn'), ('garbage truck', 'dustcart'), ('gas pump', 'gasoline pump', 'petrol pump', 'island dispenser'), ('golf ball',), ('parachute', 'chute')]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "img_size = 160\n",
    "patch_size = 16\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(10),  # Random rotation by 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color Jitter\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Vertical flip with 50% probability\n",
    "    transforms.RandomResizedCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    trans,\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    trans,\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = torchvision.datasets.Imagenette(root=dataset_path, split='train', transform=transform_train) #download=True,\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.Imagenette(root=dataset_path, split='val', transform=transform_validation) #download=True\n",
    "val_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_base = 768\n",
    "dim_small = 384\n",
    "\n",
    "heads_base = 12\n",
    "heads_small = 6\n",
    "\n",
    "n_patch_75 = [100, 100, 100, 75, 75, 75, 56, 56, 56, 42, 42, 42]\n",
    "n_patch_50 = [100, 100, 100, 50, 50, 50, 25, 25, 25, 12, 12, 12]\n",
    "\n",
    "n_patch_75_PM = [(2,75),(5,56),(8,42)]\n",
    "n_patch_50_PM = [(2,50),(5,25),(8,12)]\n",
    "\n",
    "epoche = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViTnet_base = SimpleViT(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_base,\n",
    "        depth = 12,\n",
    "        heads = heads_base,\n",
    "        mlp_dim = dim_base*4,\n",
    ")\n",
    "\n",
    "\n",
    "ViTnet_small = SimpleViT(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_small,\n",
    "        depth = 12,\n",
    "        heads = heads_small,\n",
    "        mlp_dim = dim_small*4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_75%\", model = ViTnet_base)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_50%\", model = ViTnet_base)\n",
    "\n",
    "\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_75%\", model = ViTnet_small)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_50%\", model = ViTnet_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViTnet_base = TRAM(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_base,\n",
    "        depth = 12,\n",
    "        heads = heads_base,\n",
    "        mlp_dim = dim_base*4,\n",
    ")\n",
    "\n",
    "\n",
    "ViTnet_small = TRAM(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_small,\n",
    "        depth = 12,\n",
    "        heads = heads_small,\n",
    "        mlp_dim = dim_small*4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "INIZIO TRAINING\n",
      "[    0/ 9469 (  0%)]  Loss: 2.3164\n",
      "[ 6400/ 9469 ( 68%)]  Loss: 1.9757\n",
      "[0.0001]\n",
      "INIZIO VALIDATION\n",
      "\n",
      "Average test loss: 1.9280  Accuracy: 1228/ 3925 (31.29%)\n",
      "\n",
      "Execution time: 115.46 seconds\n",
      "########################################\n",
      "Total Time: 115.45905828475952\n",
      "Epoch: 1/1\n",
      "INIZIO TRAINING\n",
      "[    0/ 9469 (  0%)]  Loss: 2.0651\n",
      "[ 6400/ 9469 ( 68%)]  Loss: 1.9515\n",
      "[0.0001]\n",
      "INIZIO VALIDATION\n",
      "\n",
      "Average test loss: 1.8440  Accuracy: 1379/ 3925 (35.13%)\n",
      "\n",
      "Execution time: 85.03 seconds\n",
      "########################################\n",
      "Total Time: 85.03131008148193\n",
      "Epoch: 1/1\n",
      "INIZIO TRAINING\n",
      "[    0/ 9469 (  0%)]  Loss: 2.4202\n",
      "[ 6400/ 9469 ( 68%)]  Loss: 2.0073\n",
      "[0.0001]\n",
      "INIZIO VALIDATION\n",
      "\n",
      "Average test loss: 1.9115  Accuracy: 1259/ 3925 (32.08%)\n",
      "\n",
      "Execution time: 38.62 seconds\n",
      "########################################\n",
      "Total Time: 38.6196825504303\n",
      "Epoch: 1/1\n",
      "INIZIO TRAINING\n",
      "[    0/ 9469 (  0%)]  Loss: 1.7858\n",
      "[ 6400/ 9469 ( 68%)]  Loss: 1.6933\n",
      "[0.0001]\n",
      "INIZIO VALIDATION\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m ViT_train_test_save(dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimaginette_SimpleViTBase_TRAM_50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m ViTnet_base, n_patch \u001b[38;5;241m=\u001b[39m n_patch_50)\n\u001b[1;32m      5\u001b[0m ViT_train_test_save(dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimaginette_SimpleViTSmall_TRAM_75\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m ViTnet_small, n_patch \u001b[38;5;241m=\u001b[39m n_patch_75)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mViT_train_test_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimaginette_SimpleViTSmall_TRAM_50\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mViTnet_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_patch_50\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mViT_train_test_save\u001b[0;34m(dataset_name, model, n_patch)\u001b[0m\n\u001b[1;32m      9\u001b[0m     _, _, validation_acc, epoch_time \u001b[38;5;241m=\u001b[39m train_validation(model, optimizer, train_loader, val_loader, dataset_name, epoche, device \u001b[38;5;241m=\u001b[39m device, scheduler \u001b[38;5;241m=\u001b[39m scheduler)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     _, _, validation_acc, epoch_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_validation_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoche\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_patch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39minitial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [tensor\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m validation_acc],\n\u001b[1;32m     15\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_time\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch_time\n\u001b[1;32m     16\u001b[0m                    })\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mtrain_validation_sampling\u001b[0;34m(model, optimizer, train_loader, validation_loader, dataset_name, epoche, device, n_patch, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m train_iter_sampling(model, optimizer, train_loader, tr_loss, device, n_patch, scheduler)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINIZIO VALIDATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_patch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m ts_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     17\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mevaluate_sampling\u001b[0;34m(model, optimizer, data_load, loss_val, device, n_patch)\u001b[0m\n\u001b[1;32m      7\u001b[0m tloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m data_load:\n\u001b[1;32m     12\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_TRAM_75%\", model = ViTnet_base, n_patch = n_patch_75)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_TRAM_50%\", model = ViTnet_base, n_patch = n_patch_50)\n",
    "\n",
    "\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_TRAM_75%\", model = ViTnet_small, n_patch = n_patch_75)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_TRAM_50%\", model = ViTnet_small, n_patch = n_patch_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchMerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transformer_PatchMerger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ViTnet_base \u001b[38;5;241m=\u001b[39m \u001b[43mPatchMerger\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheads_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim_base\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m ViTnet_small \u001b[38;5;241m=\u001b[39m PatchMerger(\n\u001b[1;32m     13\u001b[0m         image_size \u001b[38;5;241m=\u001b[39m img_size,\n\u001b[1;32m     14\u001b[0m         patch_size \u001b[38;5;241m=\u001b[39m patch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         mlp_dim \u001b[38;5;241m=\u001b[39m dim_small\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/JupyterHub/dtraini/Github/TRAM/Code/Scratch/SimpleViT/PatchMerger.py:127\u001b[0m, in \u001b[0;36mPatchMerger.__init__\u001b[0;34m(self, image_size, patch_size, num_classes, patch_merge_layers, dim, depth, heads, mlp_dim, channels, dim_head)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_patch_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    115\u001b[0m     Rearrange(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb c (h p1) (w p2) -> b (h w) (p1 p2 c)\u001b[39m\u001b[38;5;124m\"\u001b[39m, p1 \u001b[38;5;241m=\u001b[39m patch_height, p2 \u001b[38;5;241m=\u001b[39m patch_width),\n\u001b[1;32m    116\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm(patch_dim),\n\u001b[1;32m    117\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(patch_dim, dim),\n\u001b[1;32m    118\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm(dim),\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding \u001b[38;5;241m=\u001b[39m posemb_sincos_2d(\n\u001b[1;32m    122\u001b[0m     h \u001b[38;5;241m=\u001b[39m image_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_height,\n\u001b[1;32m    123\u001b[0m     w \u001b[38;5;241m=\u001b[39m image_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_width,\n\u001b[1;32m    124\u001b[0m     dim \u001b[38;5;241m=\u001b[39m dim,\n\u001b[1;32m    125\u001b[0m ) \n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer_PatchMerger\u001b[49m(dim, depth, heads, dim_head, mlp_dim, patch_merge_layers)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Transformer_PatchMerger' is not defined"
     ]
    }
   ],
   "source": [
    "ViTnet_base = PatchMerger(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_base,\n",
    "        depth = 12,\n",
    "        heads = heads_base,\n",
    "        mlp_dim = dim_base*4,\n",
    ")\n",
    "\n",
    "\n",
    "ViTnet_small = PatchMerger(\n",
    "        image_size = img_size,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim_small,\n",
    "        depth = 12,\n",
    "        heads = heads_small,\n",
    "        mlp_dim = dim_small*4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_PatchMerger_75%\", model = ViTnet_base, n_patch = n_patch_75_PM)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTBase_PatchMerger_50%\", model = ViTnet_base, n_patch = n_patch_50_PM)\n",
    "\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_PatchMerger_75%\", ViTnet_small, n_patch = n_patch_75_PM)\n",
    "ViT_train_test_save(dataset_name = \"imaginette_SimpleViTSmall_PatchMerger_50%\", ViTnet_small, n_patch = n_patch_50_PM)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rXC93PvZHALw",
    "-WOxO6GNga2i",
    "xTio5oHEG9sv",
    "ng75uqlIgCvl",
    "SWAh2lXxHQhL",
    "QTJ-jgfrQ68o"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
