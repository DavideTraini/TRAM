{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rXC93PvZHALw",
   "metadata": {
    "id": "rXC93PvZHALw"
   },
   "source": [
    "# Import e path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bBKm4O3pn19s",
   "metadata": {
    "id": "bBKm4O3pn19s"
   },
   "outputs": [],
   "source": [
    "results_save_path = \"../Results/\"\n",
    "dataset_path = \"../Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJ19eMSXuFjL",
   "metadata": {
    "id": "fJ19eMSXuFjL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from torchvision.utils import make_grid\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import timm\n",
    "\n",
    "from TRAM import TRAM\n",
    "from ViT import ViT\n",
    "from TopK import TopKViT\n",
    "from PatchMergerViT import PatchMergerViT\n",
    "from ATSViT import ATSViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xi1SBM-8TRqg",
   "metadata": {
    "id": "Xi1SBM-8TRqg"
   },
   "source": [
    "<h3> Validi per ogni modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIBVHD-TTU6u",
   "metadata": {
    "id": "zIBVHD-TTU6u"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qL0i8NnGpUPc",
   "metadata": {
    "id": "qL0i8NnGpUPc"
   },
   "outputs": [],
   "source": [
    "# Verifica se la GPU Ã¨ disponibile\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sG2VBWH-HmoI",
   "metadata": {
    "id": "sG2VBWH-HmoI"
   },
   "source": [
    "# Funzioni per train e validation del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XlttuN1BQmov",
   "metadata": {
    "id": "XlttuN1BQmov"
   },
   "source": [
    "<h3> Funzione per il Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NslZrU5FGFWo",
   "metadata": {
    "id": "NslZrU5FGFWo"
   },
   "outputs": [],
   "source": [
    "def train_iter(model, optimz, data_load, loss_val, device):\n",
    "    samples = len(data_load.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_load):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimz.zero_grad()\n",
    "        out = functional.log_softmax(model(data), dim=1)\n",
    "        loss = functional.nll_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimz.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "    loss_val.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u5ZT8m05QqHe",
   "metadata": {
    "id": "u5ZT8m05QqHe"
   },
   "source": [
    "<h3> Funzione per la validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SiazmHl-QtQ_",
   "metadata": {
    "id": "SiazmHl-QtQ_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, optimizer, data_load, loss_val, device):\n",
    "    model.eval()\n",
    "\n",
    "    samples = len(data_load.dataset)\n",
    "    # predizioni corrette\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = functional.log_softmax(model(data), dim=1)\n",
    "            loss = functional.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    aloss = tloss / samples\n",
    "    loss_val.append(aloss)\n",
    "    acc = (100.0 * csamp / samples).cpu()\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(samples) + ' (' +\n",
    "          '{:4.2f}'.format(acc) + '%)\\n')\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cGMNfZXkII",
   "metadata": {
    "id": "e6cGMNfZXkII"
   },
   "source": [
    "<h3> Funzione per allenare e validare il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9XmFO4NXfvR",
   "metadata": {
    "id": "S9XmFO4NXfvR"
   },
   "outputs": [],
   "source": [
    "def train_validation(model, optimizer, train_loader, validation_loader, nome_file, epoche, device):\n",
    "  tr_loss, ts_loss, ts_acc, epoch_time_list = [], [], [], []\n",
    "\n",
    "  for epoch in range(1, epoche + 1):\n",
    "\n",
    "      start_time = time.time()\n",
    "\n",
    "      print(f'Epoch: {epoch}/{epoche}')\n",
    "      print(\"INIZIO TRAINING\")\n",
    "      train_iter(model, optimizer, train_loader, tr_loss, device)\n",
    "      print(\"INIZIO VALIDATION\")\n",
    "      acc = evaluate(model, optimizer, validation_loader, ts_loss, device)          \n",
    "\n",
    "      ts_acc.append(acc)\n",
    "\n",
    "\n",
    "      epoch_time = time.time() - start_time\n",
    "      epoch_time_list.append(epoch_time)\n",
    "\n",
    "      print('Execution time:', '{:5.2f}'.format(epoch_time), 'seconds')\n",
    "      print(\"#\"*40)\n",
    "\n",
    "  return tr_loss, ts_loss, ts_acc, epoch_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429642e-2651-40ba-8eec-53cbe336c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patch_list(total_patches, cut):\n",
    "    # Calcola l'importo scontato per ogni blocco di 3 elementi\n",
    "    discounted_patches = total_patches\n",
    "    patch_list = []\n",
    "    for i in range(12):\n",
    "        if i % 3 == 0 and i != 0:\n",
    "            discounted_patches = int(discounted_patches * (cut / 100))\n",
    "        patch_list.append(discounted_patches)\n",
    "    return patch_list\n",
    "\n",
    "\n",
    "\n",
    "def performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size):\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    patch = create_patch_list(total_patches, cut)\n",
    "\n",
    "    if modello == 'PatchMergerViT':\n",
    "        patch = [(2,patch[3]),(5,patch[6]),(8,patch[9])]\n",
    "\n",
    "    model = get_weights(modello, patch, num_classes, model_timm, dim, heads, img_size, patch_size)\n",
    "    print(f'{results_save_path}{nome_file}/{modello}_{size}_{cut}%.csv')\n",
    "\n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    model.to(device)\n",
    "\n",
    "    # definiamo l'ottimizzatore\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss, validation_loss, validation_acc, epoch_time = train_validation(model, optimizer, train_loader, validation_loader, nome_file, epoche, device = device)\n",
    "\n",
    "    df = pd.DataFrame({'train_loss': train_loss,\n",
    "                       'validation_loss': validation_loss,\n",
    "                       'validation_acc': [tensor.item() for tensor in validation_acc],\n",
    "                       'epoch_time': epoch_time\n",
    "                       })\n",
    "\n",
    "    return df.to_csv(f'{results_save_path}{nome_file}/{modello}_{size}_{cut}%.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563efbf-8d4e-422c-aab4-0ad594a388bc",
   "metadata": {
    "id": "3563efbf-8d4e-422c-aab4-0ad594a388bc"
   },
   "source": [
    "# Funzioni per importare i pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1b515-8953-4bcb-b874-b7dc51abf7c4",
   "metadata": {
    "id": "1fc1b515-8953-4bcb-b874-b7dc51abf7c4"
   },
   "outputs": [],
   "source": [
    "def get_weights(modello, n_patch, num_classes, model_timm, dim, heads, image_size, patch_size):\n",
    "\n",
    "    # Definizione dei parametri del modello ViT\n",
    "    depth = 12\n",
    "    mlp_dim = dim * 4\n",
    "\n",
    "    # Creazione del modello ViT utilizzando vit_pytorch\n",
    "    if modello == 'TRAM':\n",
    "\n",
    "        model_vit = TRAM(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            n_patch = n_patch\n",
    "        )\n",
    "\n",
    "    elif modello == 'ATSViT':\n",
    "\n",
    "        model_vit = ATSViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            max_tokens_per_depth = n_patch\n",
    "        )\n",
    "\n",
    "    elif modello == 'PatchMergerViT':\n",
    "\n",
    "        model_vit = PatchMergerViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            patch_merge_layers = n_patch\n",
    "        )\n",
    "\n",
    "    elif modello == 'TopKViT':\n",
    "\n",
    "        model_vit = TopKViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            n_patch = n_patch\n",
    "        )\n",
    "\n",
    "    elif modello == 'ViT':\n",
    "\n",
    "        model_vit = ViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "        )\n",
    "\n",
    "    # Ottieni i pesi del modello\n",
    "    model_timm_weights = model_timm.state_dict()\n",
    "\n",
    "    # Ottieni i pesi del modello\n",
    "    model_vit_weights = model_vit.state_dict()\n",
    "\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for elem_timm, elem_vit in zip(model_timm.blocks, model_vit.transformer.layers):\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.norm.weight'] = model_timm_weights.pop(f'blocks.{i}.norm1.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.norm.bias'] = model_timm_weights.pop(f'blocks.{i}.norm1.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_qkv.weight'] = model_timm_weights.pop(f'blocks.{i}.attn.qkv.weight')\n",
    "        model_timm_weights.pop(f'blocks.{i}.attn.qkv.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_out.0.weight'] = model_timm_weights.pop(f'blocks.{i}.attn.proj.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_out.0.bias'] = model_timm_weights.pop(f'blocks.{i}.attn.proj.bias')\n",
    "\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.0.weight'] = model_timm_weights.pop(f'blocks.{i}.norm2.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.0.bias'] = model_timm_weights.pop(f'blocks.{i}.norm2.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.1.weight'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc1.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.1.bias'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc1.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.4.weight'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc2.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.4.bias'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc2.bias')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    model_timm_weights[f'mlp_head.weight'] = model_timm_weights.pop(f'head.weight')\n",
    "    model_timm_weights[f'mlp_head.bias'] = model_timm_weights.pop(f'head.bias')\n",
    "\n",
    "    model_timm_weights[f'transformer.norm.weight'] = model_timm_weights.pop(f'norm.weight')\n",
    "    model_timm_weights[f'transformer.norm.bias'] = model_timm_weights.pop(f'norm.bias')\n",
    "\n",
    "    model_timm_weights[f'pos_embedding'] = model_timm_weights.pop(f'pos_embed')\n",
    "\n",
    "    if modello == 'PatchMergerViT':\n",
    "        for elem in [\"cls_token\", \"mlp_head.weight\", \"mlp_head.bias\"]:\n",
    "            model_timm_weights.pop(elem)\n",
    "        for elem in [\"transformer.patch_mergers.2.queries\", \"transformer.patch_mergers.2.norm.weight\", \"transformer.patch_mergers.2.norm.bias\", \"transformer.patch_mergers.5.queries\", \"transformer.patch_mergers.5.norm.weight\", \"transformer.patch_mergers.5.norm.bias\", \"transformer.patch_mergers.8.queries\", \"transformer.patch_mergers.8.norm.weight\", \"transformer.patch_mergers.8.norm.bias\", \"mlp_head.1.weight\", \"mlp_head.1.bias\"]:\n",
    "            model_timm_weights[elem] = model_vit_weights[elem]\n",
    "\n",
    "    if modello == 'ATSViT':\n",
    "        model_timm_weights[\"mlp_head.0.weight\"] = model_timm_weights.pop('transformer.norm.weight')\n",
    "        model_timm_weights[\"mlp_head.0.bias\"] = model_timm_weights.pop(\"transformer.norm.bias\")\n",
    "        model_timm_weights[\"mlp_head.1.weight\"] = model_timm_weights.pop(\"mlp_head.weight\")\n",
    "        model_timm_weights[\"mlp_head.1.bias\"] = model_timm_weights.pop(\"mlp_head.bias\")\n",
    "\n",
    "    model_vit.load_state_dict(model_timm_weights)\n",
    "\n",
    "\n",
    "    return model_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rVWB4rvHEwb",
   "metadata": {
    "id": "4rVWB4rvHEwb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imagenette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng75uqlIgCvl",
   "metadata": {
    "id": "ng75uqlIgCvl"
   },
   "source": [
    "## Dataset e Modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diI87IeqHIE5",
   "metadata": {
    "id": "diI87IeqHIE5"
   },
   "source": [
    "<h3> Parametri del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XLxr6JMmgP0g",
   "metadata": {
    "id": "XLxr6JMmgP0g"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "nome_file = \"Imagenette_pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SWAh2lXxHQhL",
   "metadata": {
    "id": "SWAh2lXxHQhL"
   },
   "source": [
    "## Divisione tra train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TPPcrxFQHS-r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15123,
     "status": "ok",
     "timestamp": 1693838180517,
     "user": {
      "displayName": "davide traini",
      "userId": "10222462233639432801"
     },
     "user_tz": -120
    },
    "id": "TPPcrxFQHS-r",
    "outputId": "469354a6-deeb-4431-9379-9f7c361a41d3"
   },
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = torchvision.datasets.Imagenette(root=dataset_path, split='train', transform=transform_train) #download=True,\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.Imagenette(root=dataset_path, split='val', transform=transform_validation) #download=True\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QTJ-jgfrQ68o",
   "metadata": {
    "id": "QTJ-jgfrQ68o"
   },
   "source": [
    "## Inizializzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c331f-302c-4bc8-bfc1-9f4303dda07b",
   "metadata": {
    "id": "085c331f-302c-4bc8-bfc1-9f4303dda07b"
   },
   "outputs": [],
   "source": [
    "epoche = 5\n",
    "learning_rate = 0.0001\n",
    "num_classes = 10\n",
    "cut_list = [75, 50, 25]\n",
    "total_patches = int(img_size/patch_size)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93912178-a077-4280-820d-1dee61ec836f",
   "metadata": {
    "id": "93912178-a077-4280-820d-1dee61ec836f"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da8609-6fdf-41b8-b80b-4c7a87754aef",
   "metadata": {
    "id": "18da8609-6fdf-41b8-b80b-4c7a87754aef"
   },
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_base_patch16_224.augreg2_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 768\n",
    "heads = 12\n",
    "\n",
    "size = 'Base'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262681ba-1d4a-475c-ba66-2bb179fdc4bf",
   "metadata": {
    "id": "262681ba-1d4a-475c-ba66-2bb179fdc4bf",
    "outputId": "fb13cf32-8903-4f93-a510-ffac343b1985"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'TRAM'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90438aa1-16e1-4c59-b217-21fd2d0dac10",
   "metadata": {
    "id": "90438aa1-16e1-4c59-b217-21fd2d0dac10"
   },
   "source": [
    "## Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750a366-ec47-42c0-ba0e-e5be4b3c0452",
   "metadata": {
    "id": "0750a366-ec47-42c0-ba0e-e5be4b3c0452"
   },
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_small_patch16_224.augreg_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 384\n",
    "heads = 6\n",
    "\n",
    "size = 'Small'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e8804-0204-4793-9db2-a452640f630c",
   "metadata": {
    "id": "417e8804-0204-4793-9db2-a452640f630c"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'TRAM'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UFWuDUWl-RJM",
   "metadata": {
    "id": "UFWuDUWl-RJM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dAQGJz_I-RJX",
   "metadata": {
    "id": "dAQGJz_I-RJX"
   },
   "source": [
    "## Dataset e Modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hAf8diAn-RJX",
   "metadata": {
    "id": "hAf8diAn-RJX"
   },
   "source": [
    "<h3> Parametri del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MC-OAz5r-RJX",
   "metadata": {
    "id": "MC-OAz5r-RJX"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "nome_file = \"CIFAR10_pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8OgeqcGJ-RJX",
   "metadata": {
    "id": "8OgeqcGJ-RJX"
   },
   "source": [
    "## Divisione tra train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7s563kjg-RJX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15123,
     "status": "ok",
     "timestamp": 1693838180517,
     "user": {
      "displayName": "davide traini",
      "userId": "10222462233639432801"
     },
     "user_tz": -120
    },
    "id": "7s563kjg-RJX",
    "outputId": "469354a6-deeb-4431-9379-9f7c361a41d3"
   },
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True, transform=transform_train) #download=True,\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.CIFAR10(root=dataset_path, train=False, transform=transform_validation) #download=True\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-3cFj3qc-RJY",
   "metadata": {
    "id": "-3cFj3qc-RJY"
   },
   "source": [
    "## Inizializzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_EiSlPaO-RJY",
   "metadata": {
    "id": "_EiSlPaO-RJY"
   },
   "outputs": [],
   "source": [
    "epoche = 5\n",
    "learning_rate = 0.0001\n",
    "num_classes = 10\n",
    "cut_list = [\n",
    "            75,\n",
    "            50,\n",
    "           ]\n",
    "total_patches = int(img_size/patch_size)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pS3Pb1qX-RJY",
   "metadata": {
    "id": "pS3Pb1qX-RJY"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pzEK3niF-RJY",
   "metadata": {
    "id": "pzEK3niF-RJY"
   },
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_base_patch16_224.augreg2_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 768\n",
    "heads = 12\n",
    "\n",
    "size = 'Base'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yLE3DU4J-RJY",
   "metadata": {
    "id": "yLE3DU4J-RJY",
    "outputId": "fb13cf32-8903-4f93-a510-ffac343b1985"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'TRAM'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uKXc2g_R-RJZ",
   "metadata": {
    "id": "uKXc2g_R-RJZ"
   },
   "source": [
    "## Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QNBMrhzx-RJZ",
   "metadata": {
    "id": "QNBMrhzx-RJZ"
   },
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_small_patch16_224.augreg_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 384\n",
    "heads = 6\n",
    "\n",
    "cut_list = [\n",
    "            75,\n",
    "            50\n",
    "            ]\n",
    "\n",
    "size = 'Small'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5Bus2hE-RJZ",
   "metadata": {
    "id": "s5Bus2hE-RJZ"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'TRAM'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XjT_hu7A4o20",
   "metadata": {
    "id": "XjT_hu7A4o20",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_WoNPo064o2_",
   "metadata": {
    "id": "_WoNPo064o2_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset e Modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCbciD6H4o2_",
   "metadata": {
    "id": "cCbciD6H4o2_"
   },
   "source": [
    "<h3> Parametri del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-kt71c934o2_",
   "metadata": {
    "id": "-kt71c934o2_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "nome_file = \"FMNIST_pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CcY6ZVHY4o2_",
   "metadata": {
    "id": "CcY6ZVHY4o2_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Divisione tra train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6XOmYi494o2_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15123,
     "status": "ok",
     "timestamp": 1693838180517,
     "user": {
      "displayName": "davide traini",
      "userId": "10222462233639432801"
     },
     "user_tz": -120
    },
    "id": "6XOmYi494o2_",
    "outputId": "469354a6-deeb-4431-9379-9f7c361a41d3"
   },
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "\n",
    "trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    trans,\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    trans,\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = torchvision.datasets.FashionMNIST(root=dataset_path, train=True, transform=transform_train) #download=True,\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.FashionMNIST(root=dataset_path, train=False, transform=transform_validation) #download=True\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dG-t0H4o3A",
   "metadata": {
    "id": "c0dG-t0H4o3A",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inizializzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acolSbCe4o3A",
   "metadata": {
    "id": "acolSbCe4o3A"
   },
   "outputs": [],
   "source": [
    "epoche = 5\n",
    "learning_rate = 0.0001\n",
    "num_classes = 10\n",
    "cut_list = [\n",
    "            # 75,\n",
    "            50,\n",
    "            # 25\n",
    "           ]\n",
    "total_patches = int(img_size/patch_size)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qUX5Yg294o3A",
   "metadata": {
    "id": "qUX5Yg294o3A"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktWN7OjE4o3A",
   "metadata": {
    "id": "ktWN7OjE4o3A"
   },
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_base_patch16_224.augreg2_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 768\n",
    "heads = 12\n",
    "\n",
    "size = 'Base'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v_EbBskQ4o3B",
   "metadata": {
    "id": "v_EbBskQ4o3B",
    "outputId": "fb13cf32-8903-4f93-a510-ffac343b1985"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'TRAM'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJeU_PCi4o3B",
   "metadata": {
    "id": "KJeU_PCi4o3B"
   },
   "source": [
    "## Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czqKrLZO4o3B",
   "metadata": {
    "id": "czqKrLZO4o3B"
   },
   "outputs": [],
   "source": [
    "cut_list = [\n",
    "            75,\n",
    "            50\n",
    "           ]\n",
    "\n",
    "# Definizione dei parametri del modello ViT\n",
    "model_name = 'vit_small_patch16_224.augreg_in21k_ft_in1k'\n",
    "pretrained = True\n",
    "dim = 384\n",
    "heads = 6\n",
    "\n",
    "size = 'Small'\n",
    "\n",
    "# Caricamento del modello ViT utilizzando timm\n",
    "model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zEkJcoWn4o3B",
   "metadata": {
    "id": "zEkJcoWn4o3B"
   },
   "outputs": [],
   "source": [
    "for cut in cut_list:\n",
    "\n",
    "    modello = 'PatchMergerViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'ATSViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "    modello = 'TopKViT'\n",
    "    performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)\n",
    "\n",
    "\n",
    "modello = 'ViT'\n",
    "performance(modello, total_patches, learning_rate, num_classes, epoche, model_timm, dim, heads, cut, size, img_size, patch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "485892e6-c603-41f6-b558-c6c3a11eb840",
    "rXC93PvZHALw",
    "sG2VBWH-HmoI",
    "3563efbf-8d4e-422c-aab4-0ad594a388bc",
    "xTio5oHEG9sv",
    "4rVWB4rvHEwb",
    "ng75uqlIgCvl",
    "SWAh2lXxHQhL",
    "QTJ-jgfrQ68o",
    "_WoNPo064o2_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
