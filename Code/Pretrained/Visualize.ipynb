{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rXC93PvZHALw",
   "metadata": {
    "id": "rXC93PvZHALw"
   },
   "source": [
    "# Import e path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bBKm4O3pn19s",
   "metadata": {
    "id": "bBKm4O3pn19s"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJ19eMSXuFjL",
   "metadata": {
    "id": "fJ19eMSXuFjL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from torchvision.utils import make_grid\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "from TRAM import TRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xi1SBM-8TRqg",
   "metadata": {
    "id": "Xi1SBM-8TRqg"
   },
   "source": [
    "<h3> Validi per ogni modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIBVHD-TTU6u",
   "metadata": {
    "id": "zIBVHD-TTU6u"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qL0i8NnGpUPc",
   "metadata": {
    "id": "qL0i8NnGpUPc"
   },
   "outputs": [],
   "source": [
    "# Verifica se la GPU Ã¨ disponibile\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sG2VBWH-HmoI",
   "metadata": {
    "id": "sG2VBWH-HmoI"
   },
   "source": [
    "# Funzioni per train e validation del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XlttuN1BQmov",
   "metadata": {
    "id": "XlttuN1BQmov"
   },
   "source": [
    "<h3> Funzione per il Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NslZrU5FGFWo",
   "metadata": {
    "id": "NslZrU5FGFWo"
   },
   "outputs": [],
   "source": [
    "def train_iter(model, optimz, data_load, loss_val, device):\n",
    "    samples = len(data_load.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_load):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimz.zero_grad()\n",
    "        out = functional.log_softmax(model(data), dim=1)\n",
    "        loss = functional.nll_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimz.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "    loss_val.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u5ZT8m05QqHe",
   "metadata": {
    "id": "u5ZT8m05QqHe"
   },
   "source": [
    "<h3> Funzione per la validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SiazmHl-QtQ_",
   "metadata": {
    "id": "SiazmHl-QtQ_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, optimizer, data_load, loss_val, device):\n",
    "    model.eval()\n",
    "\n",
    "    samples = len(data_load.dataset)\n",
    "    # predizioni corrette\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = functional.log_softmax(model(data), dim=1)\n",
    "            loss = functional.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    aloss = tloss / samples\n",
    "    loss_val.append(aloss)\n",
    "    acc = (100.0 * csamp / samples).cpu()\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(samples) + ' (' +\n",
    "          '{:4.2f}'.format(acc) + '%)\\n')\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cGMNfZXkII",
   "metadata": {
    "id": "e6cGMNfZXkII"
   },
   "source": [
    "<h3> Funzione per allenare e validare il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9XmFO4NXfvR",
   "metadata": {
    "id": "S9XmFO4NXfvR"
   },
   "outputs": [],
   "source": [
    "def train_validation(model, optimizer, train_loader, validation_loader, nome_file, epoche, device):\n",
    "  tr_loss, ts_loss, ts_acc, epoch_time_list = [], [], [], []\n",
    "\n",
    "  for epoch in range(1, epoche + 1):\n",
    "\n",
    "      start_time = time.time()\n",
    "\n",
    "      print(f'Epoch: {epoch}/{epoche}')\n",
    "      print(\"INIZIO TRAINING\")\n",
    "      train_iter(model, optimizer, train_loader, tr_loss, device)\n",
    "      print(\"INIZIO VALIDATION\")\n",
    "      acc = evaluate(model, optimizer, validation_loader, ts_loss, device)\n",
    "\n",
    "      if (not ts_acc or acc >= max(ts_acc)):\n",
    "        checkpoint = {'model_state_dict': model.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      'train_loss_state_dict': tr_loss[-1],\n",
    "                      'val_loss_state_dict': ts_loss[-1],\n",
    "                      'val_acc_state_dict': acc\n",
    "                      }\n",
    "\n",
    "      ts_acc.append(acc)\n",
    "\n",
    "\n",
    "      epoch_time = time.time() - start_time\n",
    "      epoch_time_list.append(epoch_time)\n",
    "\n",
    "      print('Execution time:', '{:5.2f}'.format(epoch_time), 'seconds')\n",
    "      print(\"#\"*40)\n",
    "\n",
    "  return tr_loss, ts_loss, ts_acc, epoch_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563efbf-8d4e-422c-aab4-0ad594a388bc",
   "metadata": {
    "id": "3563efbf-8d4e-422c-aab4-0ad594a388bc"
   },
   "source": [
    "# Funzioni per importare i pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1b515-8953-4bcb-b874-b7dc51abf7c4",
   "metadata": {
    "id": "1fc1b515-8953-4bcb-b874-b7dc51abf7c4"
   },
   "outputs": [],
   "source": [
    "def get_weights(n_patch, num_classes, model_timm, dim, heads, image_size, patch_size):\n",
    "\n",
    "\n",
    "    # Definizione dei parametri del modello ViT\n",
    "    depth = 12\n",
    "    mlp_dim = dim * 4\n",
    "\n",
    "    # Creazione del modello ViT utilizzando vit_pytorch\n",
    "    model_vit = TRAM(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            n_patch = n_patch\n",
    "    )\n",
    "\n",
    "\n",
    "    # Ottieni i pesi del modello\n",
    "    model_timm_weights = model_timm.state_dict()\n",
    "\n",
    "    # Ottieni i pesi del modello\n",
    "    model_vit_weights = model_vit.state_dict()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for elem_timm, elem_vit in zip(model_timm.blocks, model_vit.transformer.layers):\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.norm.weight'] = model_timm_weights.pop(f'blocks.{i}.norm1.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.norm.bias'] = model_timm_weights.pop(f'blocks.{i}.norm1.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_qkv.weight'] = model_timm_weights.pop(f'blocks.{i}.attn.qkv.weight')\n",
    "        model_timm_weights.pop(f'blocks.{i}.attn.qkv.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_out.0.weight'] = model_timm_weights.pop(f'blocks.{i}.attn.proj.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.0.to_out.0.bias'] = model_timm_weights.pop(f'blocks.{i}.attn.proj.bias')\n",
    "\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.0.weight'] = model_timm_weights.pop(f'blocks.{i}.norm2.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.0.bias'] = model_timm_weights.pop(f'blocks.{i}.norm2.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.1.weight'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc1.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.1.bias'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc1.bias')\n",
    "\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.4.weight'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc2.weight')\n",
    "        model_timm_weights[f'transformer.layers.{i}.1.net.4.bias'] = model_timm_weights.pop(f'blocks.{i}.mlp.fc2.bias')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    model_timm_weights[f'mlp_head.weight'] = model_timm_weights.pop(f'head.weight')\n",
    "    model_timm_weights[f'mlp_head.bias'] = model_timm_weights.pop(f'head.bias')\n",
    "\n",
    "    model_timm_weights[f'transformer.norm.weight'] = model_timm_weights.pop(f'norm.weight')\n",
    "    model_timm_weights[f'transformer.norm.bias'] = model_timm_weights.pop(f'norm.bias')\n",
    "\n",
    "    model_timm_weights[f'pos_embedding'] = model_timm_weights.pop(f'pos_embed')\n",
    "\n",
    "    model_vit.load_state_dict(model_timm_weights)\n",
    "\n",
    "\n",
    "    return model_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xTio5oHEG9sv",
   "metadata": {
    "id": "xTio5oHEG9sv"
   },
   "source": [
    "# Funzioni per il dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84DSWjouVs2N",
   "metadata": {
    "id": "84DSWjouVs2N"
   },
   "source": [
    "<h3> Funzione per creare il dataloader in base al tipo di split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zYikKwHtHBC-",
   "metadata": {
    "id": "zYikKwHtHBC-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.class_folders = [f.path for f in os.scandir(self.root_dir) if f.is_dir()]\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_to_label = {class_name: i for i, class_name in enumerate(self.class_folders)}\n",
    "\n",
    "        for class_folder, class_label in class_to_label.items():\n",
    "\n",
    "            if self.split == \"train\":\n",
    "              image_folder_path = os.path.join(class_folder, \"images\")\n",
    "              image_filenames = os.listdir(image_folder_path)\n",
    "              image_paths = [os.path.join(image_folder_path, f) for f in image_filenames]\n",
    "\n",
    "            elif self.split == \"val\":\n",
    "              image_filenames = os.listdir(class_folder)\n",
    "              image_paths = [os.path.join(class_folder, f) for f in image_filenames]\n",
    "\n",
    "            self.image_paths.extend(image_paths)\n",
    "            self.labels.extend([class_label] * len(image_filenames))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07QLRIrmV0Ax",
   "metadata": {
    "id": "07QLRIrmV0Ax"
   },
   "source": [
    "<h3> Funzioni per stampare il batch di immagini passando il dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ls6eapJsV7Zj",
   "metadata": {
    "id": "ls6eapJsV7Zj"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    imshow(make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rVWB4rvHEwb",
   "metadata": {
    "id": "4rVWB4rvHEwb"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XLxr6JMmgP0g",
   "metadata": {
    "id": "XLxr6JMmgP0g"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "nome_file = \"Imagenette_pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TPPcrxFQHS-r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15123,
     "status": "ok",
     "timestamp": 1693838180517,
     "user": {
      "displayName": "davide traini",
      "userId": "10222462233639432801"
     },
     "user_tz": -120
    },
    "id": "TPPcrxFQHS-r",
    "outputId": "469354a6-deeb-4431-9379-9f7c361a41d3"
   },
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = torchvision.datasets.Imagenette(root=dataset_path, split='train', transform=transform_train) #download=True,\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.Imagenette(root=dataset_path, split='val', transform=transform_validation) #download=True\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QTJ-jgfrQ68o",
   "metadata": {
    "id": "QTJ-jgfrQ68o"
   },
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa04b8-14ff-40ce-8da6-729a1dc445d4",
   "metadata": {
    "id": "82aa04b8-14ff-40ce-8da6-729a1dc445d4"
   },
   "outputs": [],
   "source": [
    "def create_patch_list(total_patches, cut):\n",
    "    # Calcola l'importo scontato per ogni blocco di 3 elementi\n",
    "    discounted_patches = total_patches\n",
    "    patch_list = []\n",
    "    for i in range(12):\n",
    "        if i % 3 == 0 and i != 0:\n",
    "            discounted_patches = int(discounted_patches * (cut / 100))\n",
    "        patch_list.append(discounted_patches)\n",
    "    return patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdc835-2fe7-40c1-be54-3d2c12bcbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, cut, epoche, learning_rate, image_size, patch_size):\n",
    "\n",
    "    total_patches = int(image_size/patch_size)**2\n",
    "    \n",
    "    # Definizione dei parametri del modello ViT\n",
    "    if model == 'Base':\n",
    "        model_name = 'vit_base_patch16_224.augreg_in21k_ft_in1k'\n",
    "        pretrained = True\n",
    "        dim = 768\n",
    "        heads = 12\n",
    "\n",
    "    elif model == 'Small':\n",
    "        model_name = 'vit_small_patch16_224.augreg_in21k_ft_in1k'\n",
    "        pretrained = True\n",
    "        dim = 384\n",
    "        heads = 6\n",
    "\n",
    "    elif model == 'Tiny':\n",
    "        model_name = 'vit_tiny_patch16_224.augreg2_in21k_ft_in1k'\n",
    "        pretrained = True\n",
    "        dim = 192\n",
    "        heads = 3\n",
    "\n",
    "    else:\n",
    "        print('Model not found')\n",
    "        return\n",
    "    \n",
    "    patch = create_patch_list(total_patches, cut)\n",
    "    \n",
    "    # Caricamento del modello ViT utilizzando timm\n",
    "    model_timm = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    \n",
    "    model = get_weights(patch, num_classes, model_timm, dim, heads, image_size, patch_size)\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    model.to(device)\n",
    "    \n",
    "    # definiamo l'ottimizzatore\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss, validation_loss, validation_acc, epoch_time = train_validation(model, optimizer, train_loader, validation_loader, nome_file, epoche, device = device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4236054-2eeb-4b5c-9618-05d7a892ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(patch_list, mask_size, patch_size):\n",
    "    mask = torch.zeros((mask_size, mask_size), dtype=torch.float32)\n",
    "    for patch_index in patch_list:\n",
    "        x = (patch_index // (mask_size // patch_size)) * patch_size\n",
    "        y = (patch_index % (mask_size // patch_size)) * patch_size\n",
    "        mask[x:x+patch_size, y:y+patch_size] = 1\n",
    "    return mask.to(device)\n",
    "\n",
    "\n",
    "def seleziona_indici_iterativamente(lista_originale, indici_selezione):\n",
    "    risultati = []\n",
    "    lista_attuale = lista_originale[:]\n",
    "\n",
    "    for selezione in indici_selezione:\n",
    "        indici_selezionati = [lista_attuale[i] for i in selezione]\n",
    "        risultati.append(indici_selezionati)\n",
    "\n",
    "        lista_attuale = indici_selezionati[:]\n",
    "\n",
    "    return risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a6e48-63f4-48db-a445-6b62d9049b77",
   "metadata": {},
   "source": [
    "## Train 50% and 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489c290-bb40-432b-af34-bf03c7c9ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoche = 1\n",
    "learning_rate = 0.0001\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c32b8-23bc-49fd-8eca-08d22945855d",
   "metadata": {
    "id": "1f8c32b8-23bc-49fd-8eca-08d22945855d"
   },
   "outputs": [],
   "source": [
    "cut = 75\n",
    "model = 'Base'\n",
    "model_75 = train_model(model, cut, epoche, learning_rate, image_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee98dec-1fda-4146-b973-79902d46cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 50\n",
    "model = 'Base'\n",
    "model_50 = train_model(model, cut, epoche, learning_rate, image_size, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba2d2e-539a-42c2-9b3b-47242ad88293",
   "metadata": {},
   "source": [
    "## Visualization and image saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cff8f-5f21-421f-8c28-809bbcc04fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# we want to visualize one image at a time\n",
    "test_loader = torch.utils.data.DataLoader(validationset, batch_size=1, shuffle=True, num_workers=1, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43aaa2-b443-41f3-9875-32e0ff06d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor = torch.tensor([elem for elem in mean])\n",
    "std_tensor = torch.tensor([elem for elem in std])\n",
    "plot = False\n",
    "\n",
    "for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "    if i == 50:\n",
    "        break\n",
    "\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    # Utilizzare il modello model_50\n",
    "    results_50, tokens_50 = model_50(data, return_tokens=True)\n",
    "    results_50 = functional.softmax(results_50, dim=1)\n",
    "    predicted_prob_50, predicted_idx_50 = torch.max(results_50, dim=1)\n",
    "    predicted_label_50 = trainset.classes[predicted_idx_50[0]]\n",
    "    real_label_50 = trainset.classes[target[0]]\n",
    "\n",
    "    # Utilizzare il modello model_75\n",
    "    results_75, tokens_75 = model_75(data, return_tokens=True)\n",
    "    results_75 = functional.softmax(results_75, dim=1)\n",
    "    predicted_prob_75, predicted_idx_75 = torch.max(results_75, dim=1)\n",
    "    predicted_label_75 = trainset.classes[predicted_idx_75[0]]\n",
    "    real_label_75 = trainset.classes[target[0]]\n",
    "\n",
    "    total = [i for i in range(196)]\n",
    "\n",
    "    final_50 = [elem[0].tolist() for elem in tokens_50]\n",
    "    final_75 = [elem[0].tolist() for elem in tokens_75]\n",
    "\n",
    "    indici_50 = seleziona_indici_iterativamente(total, final_50)\n",
    "    indici_75 = seleziona_indici_iterativamente(total, final_75)\n",
    "\n",
    "    result_50 = []\n",
    "    result_75 = []\n",
    "\n",
    "    for indice in indici_50:\n",
    "        mask = create_mask(indice, data.shape[2], patch_size)\n",
    "        data_modified = data[0] * std_tensor.unsqueeze(0).unsqueeze(2).unsqueeze(3).to(device) + mean_tensor.unsqueeze(0).unsqueeze(2).unsqueeze(3).to(device)\n",
    "        result_50.append(data_modified[0] * mask.unsqueeze(0).unsqueeze(1))\n",
    "\n",
    "    for indice in indici_75:\n",
    "        mask = create_mask(indice, data.shape[2], patch_size)\n",
    "        data_modified = data[0] * std_tensor.unsqueeze(0).unsqueeze(2).unsqueeze(3).to(device) + mean_tensor.unsqueeze(0).unsqueeze(2).unsqueeze(3).to(device)\n",
    "        result_75.append(data_modified[0] * mask.unsqueeze(0).unsqueeze(1))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "    for j in range(0, 4):\n",
    "        image_75 = result_75[j*3][0].cpu()\n",
    "\n",
    "        axes[j].imshow(image_75.permute(1, 2, 0), cmap=plt.cm.binary)\n",
    "\n",
    "        if j == 0:\n",
    "            axes[j].set_title(f'{real_label_75[0]}')\n",
    "\n",
    "\n",
    "        axes[j].set_xticks([])\n",
    "        axes[j].set_yticks([])\n",
    "\n",
    "        if j == 1:\n",
    "            axes[j].set_title(f'Layer 3')\n",
    "        if j == 2:\n",
    "            axes[j].set_title(f'Layer 7')\n",
    "        if j == 3:\n",
    "            axes[j].set_title(f'Layer 10')\n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(f\"Imgs/Single/{i}_75_{real_label_75[0]}.pdf\", dpi=100, bbox_inches='tight')\n",
    "    if plot == True: \n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "    for j in range(0, 4):\n",
    "        image_50 = result_50[j*3][0].cpu()\n",
    "\n",
    "        axes[j].imshow(image_50.permute(1, 2, 0), cmap=plt.cm.binary)\n",
    "        \n",
    "        axes[j].set_xticks([])\n",
    "        axes[j].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Imgs/Single/{i}_50_{real_label_50[0]}.pdf\", dpi=100, bbox_inches='tight')\n",
    "    if plot == True: \n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "\n",
    "    for j in range(0, 4):\n",
    "\n",
    "        image_75 = result_75[j*3][0].cpu()\n",
    "        axes[0, j].imshow(image_75.permute(1, 2, 0), cmap=plt.cm.binary)\n",
    "\n",
    "        image_50 = result_50[j*3][0].cpu()\n",
    "        axes[1, j].imshow(image_50.permute(1, 2, 0), cmap=plt.cm.binary)\n",
    "\n",
    "        \n",
    "        if j == 0:\n",
    "            axes[0, j].set_title(f'{real_label_75[0]}')\n",
    "\n",
    "\n",
    "        axes[0, j].set_xticks([])\n",
    "        axes[0, j].set_yticks([])\n",
    "\n",
    "        axes[1, j].set_xticks([])\n",
    "        axes[1, j].set_yticks([])\n",
    "\n",
    "        if j == 1:\n",
    "            axes[0, j].set_title(f'Layer 3')\n",
    "        if j == 2:\n",
    "            axes[0, j].set_title(f'Layer 7')\n",
    "        if j == 3:\n",
    "            axes[0, j].set_title(f'Layer 10')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Imgs/United/{i}_{real_label_50[0]}.png\", dpi=100, bbox_inches='tight')\n",
    "    if plot == True: \n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"#############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc9b3b-303f-4fa7-bcbb-3d6ef0e169be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "485892e6-c603-41f6-b558-c6c3a11eb840",
    "rXC93PvZHALw",
    "sG2VBWH-HmoI",
    "3563efbf-8d4e-422c-aab4-0ad594a388bc",
    "xTio5oHEG9sv",
    "4rVWB4rvHEwb",
    "ng75uqlIgCvl",
    "SWAh2lXxHQhL",
    "QTJ-jgfrQ68o",
    "_WoNPo064o2_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
